{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5074f517",
   "metadata": {},
   "source": [
    "# Wildlife Image Recognition - Data Collection Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bc72d",
   "metadata": {},
   "source": [
    "This project focuses on building a convolutional neural network with transfer learning in order to identify dangerous animals in the mountain west. To start, we need to collect the data from the [LiLa website](https://lila.science/datasets/nacti) using code they've provided. Then we can clean it up to prepare for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import urllib.request\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af539f6",
   "metadata": {},
   "source": [
    "The following code was provided by Lila in order to collect data from their site. It is commented out as to not accidentally start the data collection process again. It collects photos for certain designated species to include in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code written by LILA to help users access their image data. \n",
    "# # Find this code here: https://github.com/microsoft/CameraTraps/blob/master/data_management/download_lila_subset.py\n",
    "# #\n",
    "# #\n",
    "# # Example of how to download a list of files from LILA, e.g. all the files\n",
    "# # in a data set corresponding to a particular species.\n",
    "# #\n",
    "\n",
    "# #%% Constants and imports\n",
    "\n",
    "\n",
    "# # LILA camera trap master metadata file\n",
    "# metadata_url = 'http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt'\n",
    "\n",
    "# # In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset\n",
    "# datasets_of_interest = ['NACTI']\n",
    "\n",
    "# # All lower-case; we'll convert category names to lower-case when comparing\n",
    "# species_of_interest = ['ursus americanus', 'odocoileus hemionus', 'canis latrans','puma concolor', 'alces alces', 'cervus canadensis',\n",
    "#                        'procyon lotor', 'unidentified chipmunk','tamiasciurus hudsonicus', 'empty']\n",
    "\n",
    "# # We'll write images, metadata downloads, and temporary files here\n",
    "# output_dir = r'c:\\temp\\lila_downloads_by_species_final'\n",
    "# os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "# # We will demonstrate two approaches to downloading, one that loops over files\n",
    "# # and downloads directly in Python, another that uses AzCopy.\n",
    "# #\n",
    "# # AzCopy will generally be more performant and supports resuming if the \n",
    "# # transfers are interrupted.  It assumes that azcopy is on the system path.\n",
    "# use_azcopy_for_download = False\n",
    "\n",
    "# overwrite_files = False\n",
    "\n",
    "# # Number of concurrent download threads (when not using AzCopy) (AzCopy does its\n",
    "# # own magical parallelism)\n",
    "# n_download_threads = 50\n",
    "\n",
    "\n",
    "# #%% Support functions\n",
    "\n",
    "# def download_url(url, destination_filename=None, force_download=False, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Download a URL (defaulting to a temporary file)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if destination_filename is None:\n",
    "#         temp_dir = os.path.join(tempfile.gettempdir(),'lila')\n",
    "#         os.makedirs(temp_dir,exist_ok=True)\n",
    "#         url_as_filename = url.replace('://', '_').replace('.', '_').replace('/', '_')\n",
    "#         destination_filename = \\\n",
    "#             os.path.join(temp_dir,url_as_filename)\n",
    "            \n",
    "#     if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "#         print('Bypassing download of already-downloaded file {}'.format(os.path.basename(url)))\n",
    "#         return destination_filename\n",
    "    \n",
    "#     if verbose:\n",
    "#         print('Downloading file {} to {}'.format(os.path.basename(url),destination_filename),end='')\n",
    "    \n",
    "#     os.makedirs(os.path.dirname(destination_filename),exist_ok=True)\n",
    "#     urllib.request.urlretrieve(url, destination_filename)  \n",
    "#     assert(os.path.isfile(destination_filename))\n",
    "    \n",
    "#     if verbose:\n",
    "#         nBytes = os.path.getsize(destination_filename)    \n",
    "#         print('...done, {} bytes.'.format(nBytes))\n",
    "        \n",
    "#     return destination_filename\n",
    "\n",
    "\n",
    "# def download_relative_filename(url, output_base, verbose=False):\n",
    "#     \"\"\"\n",
    "#     Download a URL to output_base, preserving relative path\n",
    "#     \"\"\"\n",
    "    \n",
    "#     p = urlparse(url)\n",
    "#     # remove the leading '/'\n",
    "#     assert p.path.startswith('/'); relative_filename = p.path[1:]\n",
    "#     destination_filename = os.path.join(output_base,relative_filename)\n",
    "#     download_url(url, destination_filename, verbose=verbose)\n",
    "    \n",
    "\n",
    "# def unzip_file(input_file, output_folder=None):\n",
    "#     \"\"\"\n",
    "#     Unzip a zipfile to the specified output folder, defaulting to the same location as\n",
    "#     the input file    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     if output_folder is None:\n",
    "#         output_folder = os.path.dirname(input_file)\n",
    "        \n",
    "#     with zipfile.ZipFile(input_file, 'r') as zf:\n",
    "#         zf.extractall(output_folder)\n",
    "\n",
    "\n",
    "# #%% Download and parse the metadata file\n",
    "\n",
    "# # Put the master metadata file in the same folder where we're putting images\n",
    "# p = urlparse(metadata_url)\n",
    "# metadata_filename = os.path.join(output_dir,os.path.basename(p.path))\n",
    "# download_url(metadata_url, metadata_filename)\n",
    "\n",
    "# # Read lines from the master metadata file\n",
    "# with open(metadata_filename,'r') as f:\n",
    "#     metadata_lines = f.readlines()\n",
    "# metadata_lines = [s.strip() for s in metadata_lines]\n",
    "\n",
    "# # Parse those lines into a table\n",
    "# metadata_table = {}\n",
    "\n",
    "# for s in metadata_lines:\n",
    "    \n",
    "#     if len(s) == 0 or s[0] == '#':\n",
    "#         continue\n",
    "    \n",
    "#     # Each line in this file is name/sas_url/json_url\n",
    "#     tokens = s.split(',')\n",
    "#     assert len(tokens)==3\n",
    "#     url_mapping = {'sas_url':tokens[1],'json_url':tokens[2]}\n",
    "#     metadata_table[tokens[0]] = url_mapping\n",
    "    \n",
    "#     assert 'https' not in tokens[0]\n",
    "#     assert 'https' in url_mapping['sas_url']\n",
    "#     assert 'https' in url_mapping['json_url']\n",
    "\n",
    "\n",
    "# #%% Download and extract metadata for the datasets we're interested in\n",
    "\n",
    "# for ds_name in datasets_of_interest:\n",
    "    \n",
    "#     assert ds_name in metadata_table\n",
    "#     json_url = metadata_table[ds_name]['json_url']\n",
    "    \n",
    "#     p = urlparse(json_url)\n",
    "#     json_filename = os.path.join(output_dir,os.path.basename(p.path))\n",
    "#     download_url(json_url, json_filename)\n",
    "    \n",
    "#     # Unzip if necessary\n",
    "#     if json_filename.endswith('.zip'):\n",
    "        \n",
    "#         with zipfile.ZipFile(json_filename,'r') as z:\n",
    "#             files = z.namelist()\n",
    "#         assert len(files) == 1\n",
    "#         unzipped_json_filename = os.path.join(output_dir,files[0])\n",
    "#         if not os.path.isfile(unzipped_json_filename):\n",
    "#             unzip_file(json_filename,output_dir)        \n",
    "#         else:\n",
    "#             print('{} already unzipped'.format(unzipped_json_filename))\n",
    "#         json_filename = unzipped_json_filename\n",
    "    \n",
    "#     metadata_table[ds_name]['json_filename'] = json_filename\n",
    "    \n",
    "# # ...for each dataset of interest\n",
    "\n",
    "\n",
    "# #%% List of files we're going to download (for all data sets)\n",
    "\n",
    "# # Flat list or URLS, for use with direct Python downloads\n",
    "# urls_to_download = []\n",
    "\n",
    "# # # For use with azcopy\n",
    "# downloads_by_dataset = {}\n",
    "\n",
    "# for ds_name in datasets_of_interest:\n",
    "    \n",
    "#     json_filename = metadata_table[ds_name]['json_filename']\n",
    "#     sas_url = metadata_table[ds_name]['sas_url']\n",
    "    \n",
    "#     base_url = sas_url.split('?')[0]    \n",
    "#     assert not base_url.endswith('/')\n",
    "    \n",
    "#     sas_token = sas_url.split('?')[1]\n",
    "#     assert not sas_token.startswith('?')\n",
    "    \n",
    "#     ## Open the metadata file\n",
    "    \n",
    "#     with open(json_filename, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     categories = data['categories']\n",
    "#     for c in categories:\n",
    "#         c['name'] = c['name'].lower()\n",
    "#     category_id_to_name = {c['id']:c['name'] for c in categories}\n",
    "#     annotations = data['annotations']\n",
    "#     images = data['images']\n",
    "\n",
    "\n",
    "# #     ## Build a list of image files (relative path names) that match the target species\n",
    "\n",
    "#     category_ids = []\n",
    "    \n",
    "#     for species_name in species_of_interest:\n",
    "#         matching_categories = list(filter(lambda x: x['name'] == species_name, categories))\n",
    "#         if len(matching_categories) == 0:\n",
    "#             continue\n",
    "#         assert len(matching_categories) == 1\n",
    "#         category = matching_categories[0]\n",
    "#         category_id = category['id']\n",
    "#         category_ids.append(category_id)\n",
    "    \n",
    "#     print('Found {} matching categories for data set {}:'.format(len(category_ids),ds_name))\n",
    "    \n",
    "#     if len(category_ids) == 0:\n",
    "#         continue\n",
    "    \n",
    "#     for i_category,category_id in enumerate(category_ids):\n",
    "#         print(category_id_to_name[category_id],end='')\n",
    "#         if i_category != len(category_ids) -1:\n",
    "#             print(',',end='')\n",
    "#     print('')\n",
    "    \n",
    "#     # Retrieve all the images that match that category\n",
    "#     image_ids_of_interest = set([ann['image_id'] for ann in annotations if ann['category_id'] in category_ids])\n",
    "    \n",
    "#     print('Selected {} of {} images for dataset {}'.format(len(image_ids_of_interest),len(images),ds_name))\n",
    "    \n",
    "#     # Retrieve image file names\n",
    "#     filenames = [im['file_name'] for im in images if im['id'] in image_ids_of_interest]\n",
    "#     assert len(filenames) == len(image_ids_of_interest)\n",
    "    \n",
    "#     # Convert to URLs\n",
    "#     for fn in filenames:        \n",
    "#         url = base_url + '/' + fn\n",
    "#         urls_to_download.append(url)\n",
    "\n",
    "#     downloads_by_dataset[ds_name] = {'sas_url':sas_url,'filenames':filenames}\n",
    "    \n",
    "# # ...for each dataset\n",
    "\n",
    "#     print('Found {} images to download'.format(len(urls_to_download)))\n",
    "\n",
    "#     # Loop over files\n",
    "#     print('Downloading images for {0} without azcopy'.format(species_of_interest))\n",
    "    \n",
    "#     if n_download_threads <= 1:\n",
    "    \n",
    "#         for url in tqdm(urls_to_download):        \n",
    "#             download_relative_filename(url,output_dir,verbose=True)\n",
    "        \n",
    "#     else:\n",
    "    \n",
    "#         pool = ThreadPool(n_download_threads)        \n",
    "#         tqdm(pool.imap(lambda s: download_relative_filename(s,output_dir,verbose=False), urls_to_download), total=len(urls_to_download))\n",
    "    \n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90f47c",
   "metadata": {},
   "source": [
    "Now that we've collected the data, we can organize and clean it. This will include combining dataframes, designating our target variable, and formatting the data appropriately for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fbc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('species_download_metadata.json') as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the image data from the json format\n",
    "image_data = pd.DataFrame(data['images'])\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('nacti_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the species we want included in our modelling dataset\n",
    "species_of_interest = ['ursus americanus', 'odocoileus hemionus', 'canis latrans','puma concolor', 'alces alces', 'cervus canadensis',\n",
    "                       'procyon lotor', 'unidentified chipmunk','tamiasciurus hudsonicus', 'empty']\n",
    "\n",
    "#merge images with their metadata\n",
    "full_dataset= metadata.merge(image_data, on='id')\n",
    "#make sure we only include our species of interest\n",
    "full_dataset[full_dataset.name.isin(species_of_interest)].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split species into dangerous - our target variable - and neutral\n",
    "species_dangerous = ['ursus americanus', 'canis latrans','puma concolor', 'alces alces']\n",
    "species_neutral = ['odocoileus hemionus', 'cervus canadensis','procyon lotor', \n",
    "                   'unidentified chipmunk','tamiasciurus hudsonicus']\n",
    "empty = ['empty']\n",
    "\n",
    "#here we'll rebalance our dataset a bit \n",
    "dangerous_data= full_dataset[full_dataset.name.isin(species_dangerous)]\n",
    "\n",
    "#take 1/3 of the neutral animal data ~ 50k\n",
    "neutral_data = full_dataset[full_dataset.name.isin(species_neutral)].sample(50000)\n",
    "\n",
    "#take 1/15 of the empty data ~ 30k\n",
    "empty_data = full_dataset[full_dataset.name.isin(empty)].sample(30000)\n",
    "                             \n",
    "\n",
    "\n",
    "print(dangerous_data.shape,\n",
    "     neutral_data.shape,\n",
    "     empty_data.shape)\n",
    "\n",
    "#concat our target variable data, and neutral and empty into one dataset\n",
    "dfs = [dangerous_data, neutral_data, empty_data]\n",
    "animal_data = pd.concat(dfs).sample(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb40bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a binary column as the target variable\n",
    "def is_dangerous(row):\n",
    "    dangerous_list = ['moose', 'american black bear', 'coyote', 'cougar']\n",
    "    if row in dangerous_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "animal_data['dangerous'] = animal_data.common_name.apply(is_dangerous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a085a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(animal_data[animal_data.dangerous==1])/len(animal_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#animal_data.to_csv('smaller_animal_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd262078",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_data = animal_data.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ec8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#below we're sampling some of the images to get an understanding of the dataset\n",
    "animal_data[animal_data.filename.str.contains('part3/sub324/CA-09_0003554')]\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6473b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_df1 = animal_data.reset_index()\n",
    "animal_df1\n",
    "animal_df1[animal_df1.filename.str.contains('part1/sub108/CA-45_11_02_2015_CA-45_0012269.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bear = Image.open('species_photos/nacti-unzipped/part2/sub212/FL-19_08_04_2016_FL-19_0060889.JPG')\n",
    "bear.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc42489",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.read_csv('combined_with_pixels.csv')\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df400b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any rows that don't have pixel values\n",
    "combined1 = combined.dropna(subset=['pixels'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c7aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ef1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get proportions of each species in the dataset\n",
    "len(combined1[combined1.dangerous==1])/len(combined1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d636d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined1[combined1.common_name.str.contains('bear')==1])/len(combined1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined1[combined1.common_name.str.contains('moose')==1])/len(combined1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined1[combined1.common_name.str.contains('coyote')==1])/len(combined1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802321a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined1[combined1.common_name.str.contains('cougar')==1])/len(combined1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export this to csv so it can be used in our modeling notebook\n",
    "#combined1.to_csv('final_5000_nonulls.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
